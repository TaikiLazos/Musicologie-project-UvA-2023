---
title: "Bitterness in love music (K-pop vs J-pop)"
author: "Taiki Papandreou"
date: "2023"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: cosmo
    self_contained: false
---

```{css}
.chart-wrapper .chart-stage {
    overflow-y: auto;
}
```

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(ggplot2)
library(plotly)
library(compmus)
library(cowplot)
library(ggpubr)
```

# Beginning of the story {.storyboard data-icon="ion-ios-home"}

### Welcome to my portfolio!

<font size="6"> What is this about?</font>

Music has always been an important medium for expressing emotions, especially when it comes to matters of the heart. Romantic songs, in particular, have a way of capturing the essence of love and heartbreak in a way that resonates with people across cultures.

In this project, we aim to explore the question of who is more romantic - Japanese or Korean artists? To do this, we will be analyzing a playlist of 52 sad love songs compiled by an avid Spotify user and self-proclaimed romantic, my brother.

Using R and various visualization tools, we will delve into the playlist to gain insights into the different themes and styles of music represented by the Japanese and Korean artists. Of course, being romantic is a subjective concept, but we will be using my brother's taste in music as a benchmark for our analysis.

Ultimately, our goal is to uncover any patterns or trends in the playlist that might shed light on the question of which culture produces more romantic music. So let's put on our headphones, dive into the playlist, and see what the data has to say!

------------------------------------------------------------------------

```{r picture, echo = F, fig.cap = "DALL·E 2 generating input 'asian melancholic songs thatgive you sad feeling'", out.width = '100%', out.height = '100%'}
knitr::include_graphics("pics/intro_photo.png")
```

###  Description

<font size="6"> About My </font>

There are three playlists chosen for my project in order to answer my question.

<font size="4"> My Brother's playlist</font>

The playlist is named after a Japanese word, "切ない系" (Setsunai-kei) meaning "melancholic" or "wistful" in English. The 57-song playlist is a collection of songs that convey bittersweet sentiments through sweet vocals assisted by soft and warm instrumentals. The playlist is primarily composed of songs sung in Japanese or Korean but also includes English songs. Despite the difference in language amongst the songs included, they share the similarity in the melodies of the songs, often having sweet and charming rings. The vocals of the songs are also one of the main elements that make the playlist truly melancholic, as if the artists are whispering to the heart of the listener.

<font size="4">Top 50 South Korea & Top 50 Japan</font>

To determine which culture produces more romantic music, I have decided to use my brother's playlist of 52 sad love songs from Korea and Japan as a standard for comparison. In order to assess the relative romanticism of Korean and Japanese music, I will be comparing my brother's playlist with the Top 50 playlists from each country.

By examining the degree of overlap between my brother's playlist and the Top 50 playlists from Korea and Japan, I hope to gain insights into which country's music is more aligned with my brother's taste in romantic music. While it's important to note that my brother's playlist is not necessarily representative of all Korean and Japanese romantic music, it does provide a useful benchmark for comparison.

Of course, romanticism is a subjective concept and there may be other factors beyond just cultural origin that influence my brother's selection of songs. Nonetheless, I believe that this approach will provide a useful starting point for exploring the question of which country's music is more romantic.

------------------------------------------------------------------------

<!-- <iframe src="https://open.spotify.com/playlist/5ASx85eUYUG7QmYpiJVI4p" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe> -->

### What makes a song romantic and sad?

Explain in academical term what we consider as our target songs.

```{r, include=FALSE}
 # Loading spotify playlists
leo_playlist <- get_playlist_audio_features("", "5ASx85eUYUG7QmYpiJVI4p") |> add_audio_analysis()
japan_playlist <- get_playlist_audio_features("", "37i9dQZEVXbKXQ4mDTEBXq") |> add_audio_analysis()
korea_playlist <- get_playlist_audio_features("", "37i9dQZEVXbNxXF4SkHj9F") |> add_audio_analysis()

# Rename the playlists for my convenience
leo_playlist$playlist_name <- "My Brother's playlist" 
japan_playlist$playlist_name <- "Top 50 Japan"
korea_playlist$playlist_name <- "Top 50 South Korea"

# appending dataset into one
songs <-
  bind_rows(
    leo_playlist |> mutate(genre = "Brother"),
    japan_playlist |> mutate(genre = "Japan"),
    korea_playlist |> mutate(genre = "Korea")
  )

leo_stats <- leo_playlist |> 
  summarise(
    mean_valence = mean(leo_playlist$valence),
    mean_energy = mean(leo_playlist$energy),
    mean_dance = mean(leo_playlist$danceability),
    mean_tempo = mean(leo_playlist$tempo)
    )
japan_stats <- japan_playlist |> 
  summarise(
    mean_valence = mean(japan_playlist$valence),
    mean_energy = mean(japan_playlist$energy),
    mean_dance = mean(japan_playlist$danceability),
    mean_tempo = mean(japan_playlist$tempo)
    )
korea_stats <- korea_playlist |> 
  summarise(
    mean_valence = mean(korea_playlist$valence),
    mean_energy = mean(korea_playlist$energy),
    mean_dance = mean(korea_playlist$danceability),
    mean_tempo = mean(korea_playlist$tempo)
    )
song_stats <-
  bind_rows(
    leo_stats |> mutate(genre = "Brother"),
    japan_stats |> mutate(genre = "Japan"),
    korea_stats |> mutate(genre = "Korea")
)
```

# Visual analysis of my  {.storyboard data-icon="fa-signal"}

### Song length in Bar plot

```{r}
# Let's see the distribution of the duration of songs in each playlist
songs <- songs %>%
  mutate(track.duration_min = track.duration_ms / (1000 * 60))
ggplot(songs, aes(track.duration_min)) + geom_histogram() + facet_wrap(~ playlist_name) + ggtitle("The distribution of song length in minutes") + labs( x = "Minutes")
```

------------------------------------------------------------------------

Upon examining my brother's playlist and the Top 50 playlists from Japan and South Korea, I've noticed that there are some interesting differences in the duration of songs. Specifically, my brother's playlist consists mostly of songs around 4 minutes in length, while the Top 50 playlist from Japan has a peak at 3 minutes and a little over 4 minutes, and the Top 50 playlist from Korea peaks at 3 minutes.

### Keys of Songs

```{r}
# let's make donut charts of key_mode
library(data.table)
donut_df <- songs |> count(playlist_name, key_name, sort = TRUE)

#setDT converts to a data.table and then you calculate the fraction of each expr
#grouping by the playlist_name
setDT(donut_df)[, frac := n / sum(n), by=playlist_name]
tabulated <- donut_df[, .N, by = .(playlist_name, key_name, n)]


# BARPLOT
ggplot(donut_df, aes(x=key_name, y=n, fill=key_name)) +
  geom_bar(stat="identity")+facet_grid(~ playlist_name) + theme(
    axis.title.x=element_blank(),
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()) + labs(y = "Count", x = "Key") + ggtitle("Distribution of keys in playlist represented in bar chart")

```

------------------------------------------------------------------------

As I continue to analyze my brother's playlist and the Top 50 playlists from Japan and South Korea, I've noticed some intriguing differences in the musical keys used by the different artists.

Specifically, my brother's playlist has a notable concentration of songs in the A key. Additionally, he has an equal amount of songs in the C, C#, and D keys. In contrast, the Top 50 playlist from Japan has the highest concentration of songs in the G key, while the Top 50 playlist from Korea mainly features songs in the C and C# keys - which is consistent with the key distribution in my brother's playlist.

While the key of a song may not directly determine its level of romanticism, these observations are an interesting starting point.

### Tempo & Valence

```{r}
multi <- ggplot(songs, aes(x=tempo, y=valence, color=playlist_name)) + geom_point() + ggtitle("Tempo and Valence distribution among the playlists")
ggplotly(multi)
```

------------------------------------------------------------------------

As I continue to explore the romantic qualities of music from Japan and South Korea, I've identified two key features from Spotify API that may be particularly relevant: tempo and valence.

When examining my brother's playlist, which he curated as a collection of sad love songs, I noticed a preponderance of songs with lower tempo and valence scores. This aligns with his stated intention for the playlist and suggests that these features may be indicative of romantic music as he perceives it.

In contrast, the Top 50 playlists from Japan and South Korea exhibit a similar distribution of valence scores, centering around 0.50 and 0.75. This may reflect the similarity of popular music trends between the two countries, particularly in the realm of pop music.

However, I did observe a slightly greater prevalence of songs with low tempo and valence scores in the Top 50 playlist from South Korea than in the Top 50 playlist from Japan. Based on this observation, it's possible that South Korean music may be somewhat more romantic than Japanese music. However, it's important to note that this conclusion is based on a limited set of data and further analysis would be needed to confirm it.

### Chromagram

```{r}
day6 <-
  get_tidy_audio_analysis("322t99AIkTbD4lew9tvdgs") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
```

```{r}
day6 |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  geom_vline(xintercept = 55, color = "red") +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "All Alone - DAY6") +
  theme_minimal() +
  scale_fill_viridis_c()
```

------------------------------------------------------------------------

We are analyzing the song "All Alone" by DAY6 using the function get_tidy_audio_analysis(). The chromagram we're working with has been normalized and we're using the Euclidean distance measure. A chromagram in computational musicology is a visual representation of the distribution of energy across different musical pitches in an audio signal. You can thus see which key is prevalent in each time frame. You can see that in the beginning of the song, strong keys shift continuously beginning from C, F3, B, A and then D. After 53 second time mark A key shows to be the most important key of the song.

TIP: Implement two more songs. Explain more on the differences between different distance measures.

### Ceptogram

```{r}
zakapa <-
  get_tidy_audio_analysis("5dsDcqCBVP9cz6Ra4iQRMd") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r}
zakapa |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "As I Wished - Urban Zakapa") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

------------------------------------------------------------------------

A cepstrogram is a visual representation of the cepstral coefficients of an audio signal. Cepstral analysis is a technique used in signal processing and acoustics to analyze the spectral content of a signal by taking the inverse Fourier transform of its logarithm. In a cepstrogram, the cepstral coefficients are displayed over time, It can reveal patterns and structures in the spectral content of the signal. I made a ceptogram of the song 'As I Wished' by Urban Zakapa. I used Euclidean for normalization and root mean square as a summary statistic. We can see that lower c are appear to be more dominant. (I need a better explanation here because I am not so sure what this represents.)

### Self Similarity Matrix

```{r}
bzt <-
  get_tidy_audio_analysis("1uZ5Ulb2qfle3HbqB12vNQ") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  bzt |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  bzt |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")
```

------------------------------------------------------------------------

The two self-similarity matrices show chroma- and timbre-based self-similarity of the song 'Here I Am Again' by Yerin Baek. I was sick and I don't really know what to say. I will work on this explanation by 15th of March. But I can say that since there are no diagonal lines there are not much similar time components within the song. The song had different type of melody throughout the song but I think the overall vibe was similar so that is so strange. I also see that there is not much a clear pattern that is coming back and forth, this is also a strange phenomenon in my opinion. I need to research on this.

### Outliers and its keygrams

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}
major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
dolly <- get_tidy_audio_analysis("77bNe6jYY8yHdP7orXrz5I") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance", title="I Will Always Love You")

keshi <- get_tidy_audio_analysis("5byXSKd7QOHtVMOiptklBD") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  compmus_match_pitch_template(key_templates, "aitchison", "manhattan") |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", fill = "Distance", title = "i swear i'll never leave again")

plot_grid(dolly, keshi, labels = "AUTO")
```

------------------------------------------------------------------------

In my brother's playlist there are also western songs such as "I Will Always Love You" by Dolly Parton. In this section I will focus on this song by making a chordgram. The song is in D major but the plot shows otherwise. It depicts keys above D major. So this is strange.

Another example is "i swear i'll never leave again" by Keshi. The song is A major but it also fails to detect that. It shows Eb major is the most prevalent.

In both cases, it failed to highlight the correct chords. It highlights somewhere close but it is not accurate. (I need some explanation why this is the case in the future)

### Track-Level Summaries

```{r}
plt_3 <- songs |>
  mutate(
    popularity = track.popularity,
    name = track.name,
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) |>
  unnest(sections) |>
  ggplot(
    aes(
      name = name,
      x = tempo,
      y = tempo_section_sd,
      colour = genre
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Genre",
    size = "Duration (min)"
  )
plt_3
```

------------------------------------------------------------------------

Overall I can see that my brother's playlist has higher SD tempo value than the other two playlists. But this is probably due to the fact that my brother's playlist is smaller than the the other two.

### Histogram of Tempi

```{r}
# Change histogram plot fill colors by groups, Use semi-transparent fill
ggplot(songs, aes(x=tempo, fill=genre)) +
  geom_histogram(position="identity", alpha=0.5) + facet_grid(~ genre)

```

---

Here is a histogram of tempo of each playlists. As you can see, there is no clear similarities between these three playlists. Each playlist has different distribution in tempo. However, at just below 100 bpm both my brother's playlist and Top 50 Korea have a peak whereas Japan top 50 doesn't. This indicates that Brother's playlist is more similar to Korean than Japanese in terms of tempo.

### Tempogram

```{r}
drake <- get_tidy_audio_analysis("2Gnsof1hvZzjE1xdLRpjtf")
drake |> slice(1:00) |>
  tempogram(window_size = 4, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

```

---

Over My Dead Body by Drake has the highest tempo in my brother's playlist. Although listening to it the background instrumental is not particularly faster than other songs. However, it is suspected that rap is effecting the overall tempo of the song. I used windows size of 4 and hop size of 1 in this tempogram. We see that the bpm is mostly constant which aligns with how the song also goes. However, BPM of this song should be around 185. This is not correctly shown in the plot and I called this plot multiple times but it gave me the same result. 

# End point {.storyboard data-icon="ion-music-note"}

### What did I find?

Through our analysis of my brother's playlist and the Top 50 playlists from Japan and South Korea, we've discovered some interesting differences in the musical features that are commonly associated with romantic music.

We found that the Top 50 playlists from both countries generally exhibited a similar distribution of tempo and valence scores, reflecting similarities in pop music trends. However, the playlists diverged in other areas: the Top 50 playlist from Japan featured a higher concentration of songs in the G key and a wider range of song lengths, while the Top 50 playlist from South Korea had many songs in C and C# keys and tended to peak at 3 minutes in length.

Based on our analysis, we observed that South Korean music appears to more closely resemble my brother's playlist than Japanese music. While this is a somewhat subjective conclusion, given that it is based on my brother's personal tastes and preferences, it suggests that Koreans may be more romantic than the Japanese in his estimation.

Overall, these findings offer insights into the cultural differences in romantic music between Japan and South Korea, and may be useful for those interested in exploring these themes further or curating playlists in this genre. However, it's important to note that our conclusions are based on a relatively small sample size, and further analysis would be needed to confirm these trends.

### Future Improvements

-   Find better playlists. The results were much better when I picked romantic song playlists of South Korea and Japan instead of top 50. The problem is that they were not curated/created by Spotify.

-   Use/explore more features and maybe I can calculate similarities using KNN?
